{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b5515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import matplotlib\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "'''\n",
    "The file depth must be same\n",
    "put the path where Sub-URMP folder in Dataset_path variable. \n",
    "\n",
    "'''\n",
    "Dataset_Path = 'C:/Users/VML/ML_final/' # + Sub-URMP/chunk/train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10300771",
   "metadata": {},
   "source": [
    "# Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2319c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckDataset(path=None, train=False, filetype=None):\n",
    "    print(\"############################################\")\n",
    "    if train==True:\n",
    "        print(\"Train (\"+filetype+\")\")\n",
    "    else:\n",
    "        print(\"Test (\"+filetype+\")\")\n",
    "    list = os.listdir(path)\n",
    "    Total=0\n",
    "    for i in list:\n",
    "        temp = os.listdir(os.path.join(path,i))\n",
    "        print(i+\": \"+str(len(temp)))\n",
    "        Total += len(temp)\n",
    "    print(\"Total: \", Total)    \n",
    "    print(\"############################################\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a99e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckDataset(Dataset_Path + r\"Sub-URMP\\img\\train\", True, 'img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckDataset(Dataset_Path + r\"Sub-URMP\\chunk\\train\", True, 'wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dd1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckDataset(Dataset_Path + r\"Sub-URMP\\img\\test\", False, 'img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56012fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckDataset(Dataset_Path + r\"Sub-URMP\\chunk\\test\", False, 'wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c239a2a",
   "metadata": {},
   "source": [
    "# Change File name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62266ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChangeFileName(path, filetype):\n",
    "    j = os.listdir(path)\n",
    "    for instrumentclass in j:\n",
    "        idx = 1\n",
    "        i = os.listdir(os.path.join(path,instrumentclass))\n",
    "        renamepath = os.path.join(path,instrumentclass)\n",
    "        for eachdata in i:\n",
    "            #print(os.path.join(renamepath, eachdata))\n",
    "            os.rename(os.path.join(renamepath, eachdata), os.path.join(renamepath, str(idx)+'.'+filetype))\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed186084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCT634 Final --> your directory name\n",
    "\n",
    "ChangeFileName(Dataset_Path + r\"Sub-URMP\\chunk\\train\", 'wav')\n",
    "ChangeFileName(Dataset_Path + r\"Sub-URMP\\chunk\\test\", 'wav')\n",
    "ChangeFileName(Dataset_Path + r\"Sub-URMP\\img\\train\", 'jpg')\n",
    "ChangeFileName(Dataset_Path + r\"Sub-URMP\\img\\test\", 'jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eceb639",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e3ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubURMP(Dataset):\n",
    "    \"\"\"Customized SubURMP Dataset.\n",
    "    datset\n",
    "        `-- SubURMP\n",
    "            |-- img\n",
    "                |-- train\n",
    "                    |-- bassoon\n",
    "                        |-- 0001.jpg\n",
    "                        |-- ...\n",
    "                        `-- 1735.jpg\n",
    "                    |-- ...\n",
    "                    `-- violin\n",
    "                        |-- 0001.jpg\n",
    "                        |-- ...\n",
    "                        `-- 7430.jpg\n",
    "                `-- test\n",
    "                    |-- bassoon\n",
    "                        |-- 0001.jpg\n",
    "                        |-- ...\n",
    "                        `-- 0390.jpg\n",
    "                    |-- ...\n",
    "                    `-- violin\n",
    "                        |-- 0001.jpg\n",
    "                        |-- ...\n",
    "                        `-- 0945.jpg\n",
    "            |-- chunk\n",
    "                |-- train\n",
    "                    |-- bassoon\n",
    "                        |-- 0001.wav\n",
    "                        |-- ...\n",
    "                        `-- 1735.wav\n",
    "                    |-- ...\n",
    "                    `-- violin\n",
    "                        |-- 0001.wav\n",
    "                        |-- ...\n",
    "                        `-- 7430.wav\n",
    "                `-- test\n",
    "                    |-- bassoon\n",
    "                        |-- 0001.wav\n",
    "                        |-- ...\n",
    "                        `-- 0390.wav\n",
    "                    |-- ...\n",
    "                    `-- violin\n",
    "                        |-- 0001.wav\n",
    "                        |-- ...\n",
    "                        `-- 0945.wav\n",
    "                # Train\n",
    "                bassoon:1735\n",
    "                cello:9800\n",
    "                clarinet:8125\n",
    "                double_bass:1270\n",
    "                flute:5690\n",
    "                horn:5540\n",
    "                oboe:4505\n",
    "                sax:7615\n",
    "                trombone:8690\n",
    "                trumpet:1015\n",
    "                tuba:3285\n",
    "                viola:6530\n",
    "                violin:7430 \n",
    "\n",
    "                # Test\n",
    "                bassoon:390\n",
    "                cello:1030\n",
    "                clarinet:945\n",
    "                double_bass:1180\n",
    "                flute:925\n",
    "                horn:525\n",
    "                oboe:390\n",
    "                sax:910\n",
    "                trombone:805\n",
    "                trumpet:520\n",
    "                tuba:525\n",
    "                viola:485\n",
    "                violin:945    \n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, root, train=True, imgtransform=None, chunktransform=None):\n",
    "        super(SubURMP, self).__init__()\n",
    "        self.root = root\n",
    "        self.imgtransform = imgtransform\n",
    "        self.chunktransform = chunktransform\n",
    "        self.instruments = ['bassoon', 'cello', 'clarinet', 'double_bass', 'flute', 'horn', 'oboe', 'sax', 'trombone', 'trumpet', 'tuba', 'viola', 'violin']\n",
    "        self.how_many_train = [1735, 9800, 8125, 1270, 5690, 5540, 4505, 7615, 8690, 1015, 3285, 6530, 7430]\n",
    "        self.how_many_test = [390, 1030, 945, 1180, 925, 525, 390, 910, 805, 520, 525, 485, 945]\n",
    "        \n",
    "        if train==True:\n",
    "          dummy_img_paths = [self.root+'Sub-URMP/img/train/'+self.instruments[i]+'/'+str(j+1)+ '.jpg' for i in range(len(self.instruments)) for j in range(self.how_many_train[i])]\n",
    "          dummy_chunk_paths = [self.root+'Sub-URMP/chunk/train/'+self.instruments[i]+'/'+str(j+1)+ '.wav' for i in range(len(self.instruments)) for j in range(self.how_many_train[i])]\n",
    "        else:\n",
    "          dummy_img_paths = [self.root+'Sub-URMP/img/test/'+self.instruments[i]+'/'+str(j+1)+ '.jpg' for i in range(len(self.instruments)) for j in range(self.how_many_test[i])]\n",
    "          dummy_chunk_paths = [self.root+'Sub-URMP/chunk/test/'+self.instruments[i]+'/'+str(j+1)+ '.wav' for i in range(len(self.instruments)) for j in range(self.how_many_test[i])]\n",
    "        self.img_paths = dummy_img_paths\n",
    "        self.chunk_paths = dummy_chunk_paths\n",
    "        \n",
    "        assert isinstance(self.img_paths, list), 'Wrong type. self.paths should be list.'\n",
    "        if train is True:\n",
    "            assert len(self.img_paths) == 71230, 'There are 71,230 train images, but you have gathered %d image paths' % len(self.img_paths)\n",
    "        else:\n",
    "            assert len(self.img_paths) == 9575, 'There are 9,575 test images, but you have gathered %d image paths' % len(self.img_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):        \n",
    "        img_path = self.img_paths[idx]\n",
    "        chunk_path = self.chunk_paths[idx]\n",
    "        class_label = (self.instruments).index(img_path.split('/')[7])\n",
    "        label = torch.tensor(class_label).long()\n",
    "        \n",
    "        # img: (1080, 1920) -> (256, 256)\n",
    "        image = Image.open(img_path)\n",
    "        if self.imgtransform is not None:\n",
    "            image = self.imgtransform(image) \n",
    "       \n",
    "        # wav: sr 44.1KHz, 16 bits, stereo\n",
    "        chunk, sr = torchaudio.load(chunk_path)\n",
    "        if self.chunktransform is not None:\n",
    "            lms = self.chunktransform(chunk[0])                 \n",
    "        \n",
    "        return image, lms, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be044054",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Dataset_Path\n",
    "train = True\n",
    "imgtransform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((256,256)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "chunktransform = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=44100, n_fft=2048, hop_length=512, f_min=0.0, f_max=8000.0, n_mels=128),\n",
    "    torchaudio.transforms.AmplitudeToDB()\n",
    "    )\n",
    "\n",
    "dset = SubURMP(data_dir, train, imgtransform, chunktransform)\n",
    "print('num data:', len(dset))\n",
    "image_temp, lms_temp, label_temp = dset[1735]\n",
    "print('image shape:', image_temp.shape, '| type:', image_temp.dtype)\n",
    "print('chunk shape:', lms_temp.shape, '| type:', lms_temp.dtype)\n",
    "print('label:', label_temp) \n",
    "\n",
    "'''plt.figure()\n",
    "plt.imshow(image_temp.permute(1, 2, 0))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(lms_temp)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868eb413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "def get_dataloader(arg):\n",
    "    imgtransform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((256,256)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        ])    \n",
    "    chunktransform = nn.Sequential(\n",
    "        torchaudio.transforms.MelSpectrogram(sample_rate=44100, n_fft=2048, hop_length=512, f_min=0.0, f_max=8000.0, n_mels=128),\n",
    "        torchaudio.transforms.AmplitudeToDB()\n",
    "        )    \n",
    "    train_dataset = SubURMP(args.dataroot, train=True, imgtransform=imgtransform, chunktransform=chunktransform)    \n",
    "    train_dataset, valid_dataset = random_split(train_dataset, [int(len(train_dataset) * 0.80), len(train_dataset)-int(len(train_dataset) * 0.80)])\n",
    "    test_dataset = SubURMP(args.dataroot, train=False, imgtransform=imgtransform, chunktransform=chunktransform)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=True, drop_last=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    print(\"# of train_dataset:\", len(train_dataset))\n",
    "    print(\"# of valid_dataset:\", len(valid_dataset))\n",
    "    print(\"# of test_dataset: \", len(test_dataset))  \n",
    "    \n",
    "    return train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e883d",
   "metadata": {},
   "source": [
    "# Train / Test Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d724dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "args = edict()\n",
    "\n",
    "args.name = 'baseline'                  \n",
    "args.resume = False                  \n",
    "args.ckpt_dir = 'ckpts'              \n",
    "args.ckpt_reload = '10'              \n",
    "args.gpu = True                      \n",
    "\n",
    "# data options\n",
    "args.dataroot = Dataset_Path\n",
    "args.batch_size = 16              \n",
    "\n",
    "# training options\n",
    "args.lr = 0.001                   \n",
    "args.epoch = 50                   \n",
    "\n",
    "# tensorboard options\n",
    "args.tensorboard = True           \n",
    "args.log_dir = 'logs'             \n",
    "args.log_iter = 100               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8199543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = get_dataloader(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec6de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image, lms, label = next(iter(train_dataloader))\n",
    "\n",
    "print(image.shape)\n",
    "print(lms.shape)\n",
    "\n",
    "import model\n",
    "VAE = model.Audio2ImageVAE()\n",
    "output, mean, logvar= VAE(lms)\n",
    "\n",
    "print(\"output.shape:\",output.shape)\n",
    "\n",
    "import loss_function\n",
    "loss = loss_function.loss_function(image_temp, output, mean, logvar)\n",
    "print(\"loss\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f4fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "\n",
    "def train(args):\n",
    "    device = 'cuda' if torch.cuda.is_available() and args.gpu else 'cpu'    \n",
    "    model = OurModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epoch, args.epoch):\n",
    "        _start_time = time.time()  \n",
    "        \n",
    "        model.train()         \n",
    "        for image, lms, label in train_dataloader:\n",
    "            # Load data to gpu\n",
    "            image = image.to(device) \n",
    "            lms = lms.to(device)\n",
    "            label = label.to(device)\n",
    "            print(image.shape)\n",
    "            print(lms.shape)\n",
    "            \n",
    "            # Feed input into the network, get an output\n",
    "            result = model(image, lms, label) # example, need to change       \n",
    "            \n",
    "            # Compute loss\n",
    "            loss = model.compute_loss(label, result) # example, need to change\n",
    "            \n",
    "            # flush out the previously computed gradient \n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            # backward the computed loss.             \n",
    "            loss.backward() \n",
    "            \n",
    "            # update the network weights. \n",
    "            optimizer.step() \n",
    "            \n",
    "        t = time.time()-_start_time\n",
    "        print(f'Epoch {epoch}/{args.epoch} || train loss={loss:.4f} time={t:.3f} secs') \n",
    "    \n",
    "        _start_time = time.time()  \n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.\n",
    "            for image, lms, label in valid_dataloader:\n",
    "                image = image.to(device) \n",
    "                lms = lms.to(device)\n",
    "                label = label.to(device)\n",
    "\n",
    "                result = model(image, lms, label) \n",
    "                loss = model.compute_loss(label, result)    \n",
    "                val_loss += loss.item()*image.shape[0]\n",
    "                val_num_data += image.shape[0]\n",
    "\n",
    "            val_loss /= val_num_data\n",
    "            \n",
    "        t = time.time()-_start_time\n",
    "        print(f'Epoch {epoch}/{args.epoch} || val loss={loss:.4f} time={t:.3f} secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcdc6fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c69a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args):\n",
    "    print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311092ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cfb338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "036b5515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import matplotlib\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "'''\n",
    "The file depth must be same\n",
    "put the path where Sub-URMP folder in Dataset_path variable. \n",
    "'''\n",
    "# Dataset_Path = 'C:/Users/VML/ML_final/' # + Sub-URMP/chunk/train # Gihoon\n",
    "Dataset_Path = 'C:/Users/User/GCT634 Final/' # Jiho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10300771",
   "metadata": {},
   "source": [
    "# Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2319c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckDataset(path=None, train=False, filetype=None):\n",
    "    print(\"############################################\")\n",
    "    if train==True:\n",
    "        print(\"Train (\"+filetype+\")\")\n",
    "    else:\n",
    "        print(\"Test (\"+filetype+\")\")\n",
    "    list = os.listdir(path)\n",
    "    Total=0\n",
    "    for i in list:\n",
    "        temp = os.listdir(os.path.join(path,i))\n",
    "        print(i+\": \"+str(len(temp)))\n",
    "        Total += len(temp)\n",
    "    print(\"Total: \", Total)    \n",
    "    print(\"############################################\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff1a99e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      "Train (img)\n",
      "bassoon: 1735\n",
      "cello: 9800\n",
      "clarinet: 8125\n",
      "double_bass: 1270\n",
      "flute: 5690\n",
      "horn: 5540\n",
      "oboe: 4505\n",
      "sax: 7615\n",
      "trombone: 8690\n",
      "trumpet: 1015\n",
      "tuba: 3285\n",
      "viola: 6530\n",
      "violin: 7430\n",
      "Total:  71230\n",
      "############################################\n"
     ]
    }
   ],
   "source": [
    "CheckDataset(Dataset_Path + r\"Sub-URMP\\img\\train\", True, 'img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6071f672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      "Train (wav)\n",
      "bassoon: 1735\n",
      "cello: 9800\n",
      "clarinet: 8125\n",
      "double_bass: 1270\n",
      "flute: 5690\n",
      "horn: 5540\n",
      "oboe: 4505\n",
      "sax: 7615\n",
      "trombone: 8690\n",
      "trumpet: 1015\n",
      "tuba: 3285\n",
      "viola: 6530\n",
      "violin: 7430\n",
      "Total:  71230\n",
      "############################################\n"
     ]
    }
   ],
   "source": [
    "CheckDataset(Dataset_Path + r\"Sub-URMP\\chunk\\train\", True, 'wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2dd1e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      "Test (img)\n",
      "bassoon: 390\n",
      "cello: 1030\n",
      "clarinet: 945\n",
      "double_bass: 1180\n",
      "flute: 925\n",
      "horn: 525\n",
      "oboe: 390\n",
      "sax: 910\n",
      "trombone: 805\n",
      "trumpet: 520\n",
      "tuba: 525\n",
      "viola: 485\n",
      "violin: 945\n",
      "Total:  9575\n",
      "############################################\n"
     ]
    }
   ],
   "source": [
    "CheckDataset(Dataset_Path + r\"Sub-URMP\\img\\test\", False, 'img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56012fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      "Test (wav)\n",
      "bassoon: 390\n",
      "cello: 1030\n",
      "clarinet: 945\n",
      "double_bass: 1180\n",
      "flute: 925\n",
      "horn: 525\n",
      "oboe: 390\n",
      "sax: 910\n",
      "trombone: 805\n",
      "trumpet: 520\n",
      "tuba: 525\n",
      "viola: 485\n",
      "violin: 945\n",
      "Total:  9575\n",
      "############################################\n"
     ]
    }
   ],
   "source": [
    "CheckDataset(Dataset_Path + r\"Sub-URMP\\chunk\\test\", False, 'wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c239a2a",
   "metadata": {},
   "source": [
    "# Change File name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62266ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChangeFileName(path, filetype):\n",
    "    j = os.listdir(path)\n",
    "    for instrumentclass in j:\n",
    "        idx = 1\n",
    "        i = os.listdir(os.path.join(path,instrumentclass))\n",
    "        renamepath = os.path.join(path,instrumentclass)\n",
    "        for eachdata in i:\n",
    "            #print(os.path.join(renamepath, eachdata))\n",
    "            os.rename(os.path.join(renamepath, eachdata), os.path.join(renamepath, str(idx)+'.'+filetype))\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed186084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nChangeFileName(Dataset_Path + r\"Sub-URMP\\\\chunk\\train\", \\'wav\\')\\nChangeFileName(Dataset_Path + r\"Sub-URMP\\\\chunk\\test\", \\'wav\\')\\nChangeFileName(Dataset_Path + r\"Sub-URMP\\\\img\\train\", \\'jpg\\')\\nChangeFileName(Dataset_Path + r\"Sub-URMP\\\\img\\test\", \\'jpg\\')\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GCT634 Final --> your directory name\n",
    "'''\n",
    "ChangeFileName(Dataset_Path + r\"Sub-URMP\\chunk\\train\", 'wav')\n",
    "ChangeFileName(Dataset_Path + r\"Sub-URMP\\chunk\\test\", 'wav')\n",
    "ChangeFileName(Dataset_Path + r\"Sub-URMP\\img\\train\", 'jpg')\n",
    "ChangeFileName(Dataset_Path + r\"Sub-URMP\\img\\test\", 'jpg')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eceb639",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a89e3ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubURMP(Dataset):\n",
    "    \"\"\"Customized SubURMP Dataset.\n",
    "    datset\n",
    "        `-- SubURMP\n",
    "            |-- img\n",
    "                |-- train\n",
    "                    |-- bassoon\n",
    "                        |-- 0001.jpg\n",
    "                        |-- ...\n",
    "                        `-- 1735.jpg\n",
    "                    |-- ...\n",
    "                    `-- violin\n",
    "                        |-- 0001.jpg\n",
    "                        |-- ...\n",
    "                        `-- 7430.jpg\n",
    "                `-- test\n",
    "                    |-- bassoon\n",
    "                        |-- 0001.jpg\n",
    "                        |-- ...\n",
    "                        `-- 0390.jpg\n",
    "                    |-- ...\n",
    "                    `-- violin\n",
    "                        |-- 0001.jpg\n",
    "                        |-- ...\n",
    "                        `-- 0945.jpg\n",
    "            |-- chunk\n",
    "                |-- train\n",
    "                    |-- bassoon\n",
    "                        |-- 0001.wav\n",
    "                        |-- ...\n",
    "                        `-- 1735.wav\n",
    "                    |-- ...\n",
    "                    `-- violin\n",
    "                        |-- 0001.wav\n",
    "                        |-- ...\n",
    "                        `-- 7430.wav\n",
    "                `-- test\n",
    "                    |-- bassoon\n",
    "                        |-- 0001.wav\n",
    "                        |-- ...\n",
    "                        `-- 0390.wav\n",
    "                    |-- ...\n",
    "                    `-- violin\n",
    "                        |-- 0001.wav\n",
    "                        |-- ...\n",
    "                        `-- 0945.wav\n",
    "                # Train\n",
    "                bassoon:1735\n",
    "                cello:9800\n",
    "                clarinet:8125\n",
    "                double_bass:1270\n",
    "                flute:5690\n",
    "                horn:5540\n",
    "                oboe:4505\n",
    "                sax:7615\n",
    "                trombone:8690\n",
    "                trumpet:1015\n",
    "                tuba:3285\n",
    "                viola:6530\n",
    "                violin:7430 \n",
    "\n",
    "                # Test\n",
    "                bassoon:390\n",
    "                cello:1030\n",
    "                clarinet:945\n",
    "                double_bass:1180\n",
    "                flute:925\n",
    "                horn:525\n",
    "                oboe:390\n",
    "                sax:910\n",
    "                trombone:805\n",
    "                trumpet:520\n",
    "                tuba:525\n",
    "                viola:485\n",
    "                violin:945    \n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, root, train=True, imgtransform=None):\n",
    "        super(SubURMP, self).__init__()\n",
    "        self.root = root\n",
    "        self.imgtransform = imgtransform\n",
    "        self.instruments = ['bassoon', 'cello', 'clarinet', 'double_bass', 'flute', 'horn', 'oboe', 'sax', 'trombone', 'trumpet', 'tuba', 'viola', 'violin']\n",
    "        self.how_many_train = [1735, 9800, 8125, 1270, 5690, 5540, 4505, 7615, 8690, 1015, 3285, 6530, 7430]\n",
    "        self.how_many_test = [390, 1030, 945, 1180, 925, 525, 390, 910, 805, 520, 525, 485, 945]\n",
    "        \n",
    "        if train==True:\n",
    "          dummy_img_paths = [self.root+'Sub-URMP/img/train/'+self.instruments[i]+'/'+str(j+1)+ '.jpg' for i in range(len(self.instruments)) for j in range(self.how_many_train[i])]\n",
    "          dummy_chunk_paths = [self.root+'Sub-URMP/chunk/train/'+self.instruments[i]+'/'+str(j+1)+ '.wav' for i in range(len(self.instruments)) for j in range(self.how_many_train[i])]\n",
    "        else:\n",
    "          dummy_img_paths = [self.root+'Sub-URMP/img/test/'+self.instruments[i]+'/'+str(j+1)+ '.jpg' for i in range(len(self.instruments)) for j in range(self.how_many_test[i])]\n",
    "          dummy_chunk_paths = [self.root+'Sub-URMP/chunk/test/'+self.instruments[i]+'/'+str(j+1)+ '.wav' for i in range(len(self.instruments)) for j in range(self.how_many_test[i])]\n",
    "        self.img_paths = dummy_img_paths\n",
    "        self.chunk_paths = dummy_chunk_paths\n",
    "        \n",
    "        assert isinstance(self.img_paths, list), 'Wrong type. self.paths should be list.'\n",
    "        if train is True:\n",
    "            assert len(self.img_paths) == 71230, 'There are 71,230 train images, but you have gathered %d image paths' % len(self.img_paths)\n",
    "        else:\n",
    "            assert len(self.img_paths) == 9575, 'There are 9,575 test images, but you have gathered %d image paths' % len(self.img_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):        \n",
    "        img_path = self.img_paths[idx]\n",
    "        chunk_path = self.chunk_paths[idx]\n",
    "        class_label = (self.instruments).index(img_path.split('/')[7])\n",
    "        label = torch.tensor(class_label).long()\n",
    "        \n",
    "        # img: (1080, 1920) -> (256, 256)\n",
    "        image = Image.open(img_path)\n",
    "        if self.imgtransform is not None:\n",
    "            image = self.imgtransform(image) \n",
    "       \n",
    "        # wav: sr 44.1KHz, 16 bits, stereo\n",
    "        chunk, sr = torchaudio.load(chunk_path)               \n",
    "        \n",
    "        return image, chunk[0], label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be044054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data: 71230\n",
      "image shape: torch.Size([3, 256, 256]) | type: torch.float32\n",
      "chunk shape: torch.Size([22050]) | type: torch.float32\n",
      "label: tensor(1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nplt.figure()\\nplt.imshow(image_temp.permute(1, 2, 0))\\n\\nplt.figure()\\nplt.imshow(lms_temp)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = True\n",
    "imgtransform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((256,256)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "dset = SubURMP(Dataset_Path, train, imgtransform)\n",
    "print('num data:', len(dset))\n",
    "image_temp, lms_temp, label_temp = dset[1735]\n",
    "print('image shape:', image_temp.shape, '| type:', image_temp.dtype)\n",
    "print('chunk shape:', lms_temp.shape, '| type:', lms_temp.dtype)\n",
    "print('label:', label_temp) \n",
    "\n",
    "'''\n",
    "plt.figure()\n",
    "plt.imshow(image_temp.permute(1, 2, 0))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(lms_temp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "868eb413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataroot, batch_size):\n",
    "    imgtransform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((256,256)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        ])    \n",
    " \n",
    "    train_dataset = SubURMP(dataroot, train=True, imgtransform=imgtransform)    \n",
    "    train_dataset, valid_dataset = random_split(train_dataset, [int(len(train_dataset) * 0.80), len(train_dataset)-int(len(train_dataset) * 0.80)])\n",
    "    test_dataset = SubURMP(dataroot, train=False, imgtransform=imgtransform)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    print(\"# of train_dataset:\", len(train_dataset))\n",
    "    print(\"# of valid_dataset:\", len(valid_dataset))\n",
    "    print(\"# of test_dataset: \", len(test_dataset))  \n",
    "    \n",
    "    return train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e883d",
   "metadata": {},
   "source": [
    "# Train / Test Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aae1f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import loss_function \n",
    "\n",
    "class Runner(object):\n",
    "  def __init__(self, model, lr, sr):\n",
    "    self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "    self.learning_rate = lr\n",
    "    self.stopping_rate = sr\n",
    "    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    self.model = model.to(self.device)\n",
    "\n",
    "  # Running model for train, test and validation. mode: 'train' for training, 'eval' for validation and test\n",
    "  def run(self, dataloader, epoch, mode='TRAIN'):\n",
    "    self.model.train() if mode is 'TRAIN' else self.model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=f'{mode} Epoch {epoch:02}')  # progress bar\n",
    "    for item in pbar:\n",
    "      # Move mini-batch to the desired device.\n",
    "      image, lms, label = item\n",
    "      image = image.to(self.device) \n",
    "      lms = lms.to(self.device)\n",
    "      label = label.to(self.device)        \n",
    "      output, mean, std = self.model(lms) \n",
    "      \n",
    "      # Compute the loss.\n",
    "      loss = loss_function.loss_function(image, output, mean, std)\n",
    "      if mode is 'TRAIN':\n",
    "        # Perform backward propagation to compute gradients.\n",
    "        loss.backward()\n",
    "        # Update the parameters.\n",
    "        self.optimizer.step()\n",
    "        # Reset the computed gradients.\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "      batch_size = image.shape[0]\n",
    "      epoch_loss += batch_size * loss.item()\n",
    "    epoch_loss = epoch_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "  def test(self, dataloader):\n",
    "    epoch_loss = 0\n",
    "    return epoch_loss\n",
    "\n",
    "  def early_stop(self, loss, epoch):\n",
    "    self.scheduler.step(loss, epoch)\n",
    "    self.learning_rate = self.optimizer.param_groups[0]['lr']\n",
    "    stop = self.learning_rate < self.stopping_rate\n",
    "    return stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcbaf57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup.\n",
    "LR = 1e-1  # learning rate\n",
    "SR = 1e-5  # stopping rate\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e0cdc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train_dataset: 56984\n",
      "# of valid_dataset: 14246\n",
      "# of test_dataset:  9575\n"
     ]
    }
   ],
   "source": [
    "from model import Audio2ImageVAE\n",
    "model = Audio2ImageVAE()\n",
    "train_dataloader, valid_dataloader, test_dataloader = get_dataloader(Dataset_Path, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a7223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81869757e764306bb9e6b478c1145ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TRAIN Epoch 00:   0%|          | 0/3561 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\GCT634\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "runner = Runner(model=model, lr = LR, sr = SR)\n",
    "start = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  train_loss = runner.run(train_dataloader, epoch, 'TRAIN')\n",
    "  valid_loss = runner.run(valid_dataloader, epoch, 'VALID')\n",
    "  print(\"[Epoch %d/%d] [Train Loss: %.4f] [Valid Loss: %.4f]\" %\n",
    "        (epoch + 1, NUM_EPOCHS, train_loss, valid_loss))\n",
    "  if runner.early_stop(valid_loss, epoch + 1):\n",
    "    break\n",
    "print(\"Execution time: \"+str(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2adb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = SubURMP(Dataset_Path, False, imgtransform)\n",
    "image_temp, lms_temp, label_temp = dset[1735]\n",
    "\n",
    "#print(lms_temp.unsqueeze(0).shape)\n",
    "#print(lms_temp.unsqueeze(0).squeeze().shape)\n",
    "\n",
    "model.eval()\n",
    "output, mean, std = model(lms_temp.unsqueeze(0)) \n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image_temp.permute(1, 2, 0))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image_temp.squeeze().permute(1, 2, 0)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
